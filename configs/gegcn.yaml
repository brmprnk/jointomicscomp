GLOBAL_PARAMS:
  name: "GEGCN"
  task: 1
  num_features: 5000
  cuda: false
  random_seed: 1
  log_save_interval: 5
  early_stopping_patience: 5 # If set to 0 or <0, early stopping is turned off.
  data1: "GE"
  data2: "GCN"
  data_path1: "/home/bram/jointomicscomp/data/GE.npy"
  data_path2: "/home/bram/jointomicscomp/data/GCN.npy"
  data_features1: "/home/bram/jointomicscomp/data/GE_featureNames_DISTINCT.npy"
  data_features2: "/home/bram/jointomicscomp/data/GCN_featureNames_DISTINCT.npy"
  sample_names: "/home/bram/jointomicscomp/data/sampleNames.npy"
  cancer_type_index: "/home/bram/jointomicscomp/data/cancerType.npy"
  cancertypes: "/home/bram/jointomicscomp/data/cancerTypes.npy"
  train_ind: "/home/bram/jointomicscomp/data/task1/trainInd.npy"
  val_ind: "/home/bram/jointomicscomp/data/task1/validInd.npy"
  test_ind: "/home/bram/jointomicscomp/data/task1/testInd.npy"


BASELINE:
  task: 'impute'               # whether to `impute` one datasource from the other, `classify` (cell type), or `rank` (stage prediction)
  resultsFile: './res.pkl'
  baseline: false

MOFA+:
  # Data options
  mofa_data_path: "/home/bram/jointomicscomp/data/mofa_GE_ME_5000MAD.csv"           # MOFA requires different view of the input data
  scale_groups: false          # if groups have significantly different ranges, it is good practice to scale each group to unit variance
  scale_views: false           # if views have significantly different ranges, it is good practice to scale each view to unit variance

  # Model options
  pre_trained: ""              # Use a pre-trained model for downstream analysis
  factors: 128
  likelihoods: None            # likelihoods per view (options are "gaussian","poisson","bernoulli").
                               #     Default is None, and they are infered automatically
  spikeslab_weights: false     # use spike-slab sparsity prior in the weights? (recommended TRUE)
  ard_factors: false           # use ARD prior in the factors? (TRUE if using multiple groups)
  ard_weights: true            # use ARD prior in the weights? (TRUE if using multiple views)
  save_data: false             # boolean indicating whether to save the training data in the hdf5 file.
                               #     this is useful for some downstream analysis in R, but it can take a lot of disk space.

  # Training options
  iterations: 100              # number of iterations
  convergence_mode: "medium"   # (options are "fast", "medium", "slow"). For exploration, the fast mode is good enough.
  startELBO: 1                 # initial iteration to compute the ELBO (the objective function used to assess convergence)
  freqELBO: 1                  # frequency of computations of the ELBO (the objective function used to assess convergence)
  dropR2: null                 # minimum variance explained criteria to drop factors while training.
                               #     Default is null (None), inactive factors are not dropped during training
  verbose: false               # Verbose mode (will print, not log)


MVAE:
  # Flags
  mixture: false  # Flag that indicates if the MVAE uses Mixture-of-Experts instead of Product-of-Experts
  plot: True     # Flag for plotting training and validation losses (loss, recon loss and KL loss)

  # Model optionsmain.yaml
  pre_trained: "/home/bram/jointomicscomp/results/gegcn_moe_poe 13-09-2021 22_58_48/gegcn_moe_poe 13-09-2021 22:58:48/PoE/best_model.pth.tar"
  latent_dim: 128
  batch_size: 256
  epochs: 1000  # Large since there is early stopping
  lr: 0.0001

  # Other parameters
  log_interval: 1

MVIB:
  trainer: OmicsMIBTrainer

  # Hyperparameters
  z_dim: 128
  batch_size: 256
  epochs: 100
  input_dim1: 5000
  input_dim2: 5000

  # Optimization parameters
  optimizer_name: 'Adam'
  encoder_lr: 0.0001
  miest_lr: 0.0001

  # Parameters for the beta-scheduler
  beta_start_value: 0.001
  beta_end_value: 1
  beta_n_iterations: 100000
  beta_start_iteration: 50000

  # Other parameters
  no_logging: false
  log_interval: 5
  pre_trained: ""   # Path to a previously trained model
  overwrite: false  # When overwrite is false, this will resume training on a previous model

CGAE:
  epochs: 1000
  batch_size: 256
  latent_dim: 128
  enc1_lr: 0.0001
  dec1_lr: 0.0001
  enc2_lr: 0.0001
  dec2_lr: 0.0001
  loss_function: 'bce'
  use_batch_norm: True
  dropout_probability: 0.0
  optimizer: 'Adam'
  enc1_output_scale: 1.
  enc2_output_scale: 1.
  enc1_last_activation: 'relu'
  enc2_last_activation: 'relu'
  beta_start_value: 1
  zconstraintCoef: 1
  crossPenaltyCoef: 1


OMICADE:
  data_path: ""
